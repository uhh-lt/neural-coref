best {
  data_dir = /raid/hatzel/coref-hoi/data

  # Computation limits.
  max_top_antecedents = 10
  max_training_sentences = 3
  top_span_ratio = 0.4
  max_num_extracted_spans = 3900
  max_num_speakers = 20
  max_segment_len = 256

  # Learning
  bert_learning_rate = 1e-5
  task_learning_rate = 2e-4
  loss_type = marginalized  # {marginalized, hinge}
  mention_loss_coef = 0
  false_new_delta = 1.5  # For loss_type = hinge
  adam_eps = 1e-6
  adam_weight_decay = 1e-2
  warmup_ratio = 0.1
  max_grad_norm = 1  # Set 0 to disable clipping
  gradient_accumulation_steps = 1

  # Model hyperparameters.
  coref_depth = 1  # when 1: no higher order (except for cluster_merging)
  higher_order = attended_antecedent # {attended_antecedent, max_antecedent, entity_equalization, span_clustering, cluster_merging}
  coarse_to_fine = true
  fine_grained = true
  dropout_rate = 0.3
  ffnn_size = 1000
  ffnn_depth = 1
  cluster_ffnn_size = 1000   # For cluster_merging
  cluster_reduce = mean  # For cluster_merging
  easy_cluster_first = false  # For cluster_merging
  cluster_dloss = false  # cluster_merging
  num_epochs = 24
  feature_emb_size = 20
  max_span_width = 30
  use_metadata = true
  use_features = true
  use_segment_distance = true
  model_heads = true
  use_width_prior = true  # For mention score
  use_distance_prior = true  # For mention-ranking score
  model_type = bert

  mention_max_size = 50000

  # Other.
  conll_eval_path = ${best.data_dir}/dev.german.tuebdz_gold_conll  # gold_conll file for dev
  conll_test_path = ${best.data_dir}/test.german.tuebdz_gold_conll  # gold_conll file for test
  genres = ["n"]
  eval_frequency = 1000
  report_frequency = 100
  log_root = ${best.data_dir}

  evict = true
  unconditional_eviction_limit = 1200
  singleton_eviction_limit = 600
}

bert_euro = ${best}{
  num_docs = 2190
  bert_learning_rate = 1e-05
  task_learning_rate = 2e-4
  max_segment_len = 384
  ffnn_size = 3000
  cluster_ffnn_size = 3000
  language = german
  bert_tokenizer_name = dbmdz/bert-base-german-europeana-cased
  bert_pretrained_name_or_path = dbmdz/bert-base-german-europeana-cased
}

bert_multi = ${best}{
  data_dir = /raid/hatzel/coref-hoi/data_multi

  num_docs = 2190
  bert_learning_rate = 1e-05
  task_learning_rate = 2e-4
  max_segment_len = 384
  ffnn_size = 3000
  cluster_ffnn_size = 3000
  language = german
  bert_tokenizer_name = bert-base-multilingual-cased
  bert_pretrained_name_or_path = bert-base-multilingual-cased
}

bert_dbmdz = ${best}{
  data_dir = /raid/hatzel/coref-hoi/data_dbmdz

  warmup_ratio = 0.3
  num_docs = 2190
  bert_learning_rate = 1e-05
  task_learning_rate = 2e-4
  max_segment_len = 384
  ffnn_size = 3000
  cluster_ffnn_size = 3000
  language = german
  bert_tokenizer_name = dbmdz/bert-base-german-cased
  bert_pretrained_name_or_path = dbmdz/bert-base-german-cased
}

train_bert_euro = ${bert_euro}{
}

train_bert_multi = ${bert_multi}{
}

train_bert_dbmdz = ${bert_dbmdz}{
}

train_bert_ger = ${bert_ger}{
}

train_bert_electra = ${bert_electra}{
}

bert_ger = ${best}{
  data_dir = /raid/hatzel/coref-hoi/data_ger
  num_docs = 2190
  bert_learning_rate = 1e-05
  task_learning_rate = 2e-4
  max_segment_len = 384
  ffnn_size = 3000
  cluster_ffnn_size = 3000
  language = german
  bert_tokenizer_name = bert-base-german-cased
  bert_pretrained_name_or_path = bert-base-german-cased
}

bert_electra = ${best}{
  data_dir = /raid/hatzel/coref-hoi/data_electra
  num_docs = 2190
  bert_learning_rate = 1e-05
  task_learning_rate = 2e-4
  max_segment_len = 384
  ffnn_size = 3000
  cluster_ffnn_size = 3000
  language = german
  bert_tokenizer_name = german-nlp-group/electra-base-german-uncased
  bert_pretrained_name_or_path = german-nlp-group/electra-base-german-uncased
  model_type = electra
}

train_incremental = ${bert_dbmdz} {
  bert_learning_rate = 0  # BERT is actually detached anyways so this is not strictly necessary
  task_learning_rate = 1e-05
  incremental_learning_rate = 2e-4

  # We use a pre fine-tuned model so no need to fine tune as much
  incremental = true
  new_cluster_threshold = 0.0
  num_epochs = 10
  # Memory limit in GB
  memory_limit = 4.0
  num_epochs = 7
}


incremental = ${bert_dbmdz} {
  memory_limit = 4.0
  max_span_width = 30
  entity_representation_size = 768
  incremental = true
  new_cluster_threshold = 0.0
}

bert_dbmdz_mentions = ${bert_dbmdz}{
  num_epochs_mention = 1
  positive_class_weight = 222.5393
}
